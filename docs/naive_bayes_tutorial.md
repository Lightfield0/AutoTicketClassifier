# ğŸ¯ Naive Bayes Classifier - DetaylÄ± Ã–ÄŸrenme Rehberi

*AutoTicket Classifier projesi Ã¼zerinden AI Ã¶ÄŸrenme serisi*

---

## ğŸ“š Ä°Ã§indekiler

1. [Naive Bayes Nedir?](#1-naive-bayes-nedir)
2. [Bayes Teoremi MatematiÄŸi](#2-bayes-teoremi-matematiÄŸi)
3. [Kod Analizi](#3-kod-analizi)
4. [Pratik Ã–rnekler](#4-pratik-Ã¶rnekler)
5. [Multinomial vs Gaussian](#5-multinomial-vs-gaussian)
6. [GÃ¼Ã§lÃ¼ ve ZayÄ±f YanlarÄ±](#6-gÃ¼Ã§lÃ¼-ve-zayÄ±f-yanlarÄ±)
7. [Hiperparametre Optimizasyonu](#7-hiperparametre-optimizasyonu)

---

## 1. Naive Bayes Nedir?

**Naive Bayes**, makine Ã¶ÄŸrenmesinin en temel ve gÃ¼Ã§lÃ¼ algoritmalarÄ±ndan biridir. Ã–zellikle **metin sÄ±nÄ±flandÄ±rma** iÃ§in ideal bir baÅŸlangÄ±Ã§ algoritmasÄ±dÄ±r.

### ğŸ¯ Temel MantÄ±k
"Bu Ã¶zellikler varsa, hangi kategoriye ait olma olasÄ±lÄ±ÄŸÄ± daha yÃ¼ksek?"

### ğŸ“– Ã–rnek Senaryo
```
MÃ¼ÅŸteri MesajÄ±: "Para Ã§ekildi ama rezervasyon yok"
Naive Bayes DÃ¼ÅŸÃ¼ncesi: 
  - "para" kelimesi var â†’ %80 Ã–deme Sorunu olabilir
  - "rezervasyon" kelimesi var â†’ %60 Rezervasyon Problemi olabilir
  - SonuÃ§: En yÃ¼ksek olasÄ±lÄ±k Ã–deme Sorunu!
```

---

## 2. Bayes Teoremi MatematiÄŸi

### ğŸ§® Temel FormÃ¼l
```
P(A|B) = P(B|A) Ã— P(A) / P(B)
```

**AÃ§Ä±klama:**
- `P(A|B)` = "B olduÄŸunda A'nÄ±n olasÄ±lÄ±ÄŸÄ±" (Posterior)
- `P(B|A)` = "A olduÄŸunda B'nin olasÄ±lÄ±ÄŸÄ±" (Likelihood)
- `P(A)` = "A'nÄ±n genel olasÄ±lÄ±ÄŸÄ±" (Prior)
- `P(B)` = "B'nin genel olasÄ±lÄ±ÄŸÄ±" (Evidence)

### ğŸ« MÃ¼ÅŸteri Destek Ã–rneÄŸi
```
P(Ã–deme Sorunu | "para" kelimesi) = 
    P("para" | Ã–deme Sorunu) Ã— P(Ã–deme Sorunu) / P("para")
```

### ğŸ“Š Pratik Hesaplama Ã–rneÄŸi

**EÄŸitim Verileri:**
```
Metinler:
1. "para Ã§ekildi kart" â†’ payment_issue
2. "para Ã¶deme sorunu" â†’ payment_issue  
3. "ÅŸifre unuttum giriÅŸ" â†’ user_error
4. "hesap aÃ§amÄ±yorum" â†’ user_error
```

**1. Prior OlasÄ±lÄ±klar (Ã–nsel):**
```
P(payment_issue) = 2/4 = 0.50
P(user_error) = 2/4 = 0.50
```

**2. Likelihood Hesaplama:**

**payment_issue kategorisi iÃ§in:**
- Toplam kelime: 6 (para, Ã§ekildi, kart, para, Ã¶deme, sorunu)
- Vocabulary size: 10

```
P(para|payment_issue) = (2+1)/(6+10) = 3/16 = 0.188
P(sorunu|payment_issue) = (1+1)/(6+10) = 2/16 = 0.125
P(unuttum|payment_issue) = (0+1)/(6+10) = 1/16 = 0.062
```

**user_error kategorisi iÃ§in:**
- Toplam kelime: 5 (ÅŸifre, unuttum, giriÅŸ, hesap, aÃ§amÄ±yorum)

```
P(para|user_error) = (0+1)/(5+10) = 1/15 = 0.067
P(sorunu|user_error) = (0+1)/(5+10) = 1/15 = 0.067
P(unuttum|user_error) = (1+1)/(5+10) = 2/15 = 0.133
```

**3. Tahmin: "para sorunu"**

**payment_issue iÃ§in:**
```
Prior: P(payment_issue) = 0.50
Likelihood: P(para|payment_issue) Ã— P(sorunu|payment_issue) = 0.188 Ã— 0.125 = 0.0235
Posterior âˆ 0.50 Ã— 0.0235 = 0.01175
```

**user_error iÃ§in:**
```
Prior: P(user_error) = 0.50
Likelihood: P(para|user_error) Ã— P(sorunu|user_error) = 0.067 Ã— 0.067 = 0.0045
Posterior âˆ 0.50 Ã— 0.0045 = 0.00225
```

**Normalize edilmiÅŸ sonuÃ§:**
```
payment_issue: 0.01175 / (0.01175 + 0.00225) = 0.839 (83.9%)
user_error: 0.00225 / (0.01175 + 0.00225) = 0.161 (16.1%)

ğŸ¯ Tahmin: payment_issue
```

---

## 3. Kod Analizi

### ğŸ—ï¸ SÄ±nÄ±f YapÄ±sÄ±

```python
class NaiveBayesClassifier:
    def __init__(self, model_type='multinomial'):
        self.model_type = model_type      # Model tipi
        self.model = None                 # GerÃ§ek sklearn modeli
        self.is_trained = False           # EÄŸitildi mi?
        self.feature_names = None         # Ã–zellik isimleri
        self.classes = None               # Kategoriler
        
        # Model seÃ§imi
        if model_type == 'multinomial':
            self.model = MultinomialNB()
        elif model_type == 'gaussian':
            self.model = GaussianNB()
```

### ğŸ“ EÄŸitim Metodu

```python
def train(self, X_train, y_train, feature_names=None):
    """Modeli eÄŸit"""
    print(f"ğŸ¯ Naive Bayes ({self.model_type}) eÄŸitimi baÅŸlÄ±yor...")
    
    start_time = time.time()
    
    # Veri tipini kontrol et
    if hasattr(X_train, 'toarray'):  # Sparse matrix ise
        if self.model_type == 'gaussian':
            X_train = X_train.toarray()
    
    # ğŸ”¥ BU SATIRDA BÃœYÃœ OLUYOR!
    self.model.fit(X_train, y_train)
    
    training_time = time.time() - start_time
    
    # Model bilgilerini kaydet
    self.is_trained = True
    self.feature_names = feature_names
    self.classes = self.model.classes_
```

**`self.model.fit()` ne yapÄ±yor?**
1. Her kategoride hangi kelimelerin ne sÄ±klÄ±kla geÃ§tiÄŸini Ã¶ÄŸreniyor
2. OlasÄ±lÄ±k tablolarÄ±nÄ± hesaplÄ±yor (Bayes teoremi)
3. Gelecekte tahmin yapabilmek iÃ§in bilgileri saklÄ±yor

### ğŸ¯ Tahmin MetotlarÄ±

```python
def predict(self, X_test):
    """En yÃ¼ksek olasÄ±lÄ±klÄ± kategoriyi dÃ¶ndÃ¼rÃ¼r"""
    return self.model.predict(X_test)

def predict_proba(self, X_test):
    """Her kategori iÃ§in olasÄ±lÄ±klarÄ± dÃ¶ndÃ¼rÃ¼r"""
    return self.model.predict_proba(X_test)
```

### ğŸ† Ã–zellik Ã–nemi

```python
def get_feature_importance(self, top_n=10):
    """Hangi kelimeler hangi kategori iÃ§in Ã¶nemli?"""
    # Feature log-probabilities (sklearn'den)
    feature_log_prob = self.model.feature_log_prob_
    
    for i, class_name in enumerate(self.classes):
        # Bu kategoride en Ã¶nemli kelimeler
        class_importance = feature_log_prob[i]
        top_indices = np.argsort(class_importance)[::-1][:top_n]
```

**Ã–rnek Ã§Ä±ktÄ±:**
```
payment_issue kategorisi iÃ§in en Ã¶nemli kelimeler:
1. para (log_prob: -1.234)
2. Ã¶deme (log_prob: -1.456)
3. kart (log_prob: -1.789)
```

---

## 4. Pratik Ã–rnekler

### ğŸ§ª Demo Ã‡alÄ±ÅŸtÄ±rma

**Kod:**
```python
from models.naive_bayes import NaiveBayesClassifier
from utils.feature_extraction import FeatureExtractor

# Ã–rnek metinler
texts = [
    'Para Ã§ekildi ama rezervasyon onaylanmadÄ±',
    'Åifremi unuttum nasÄ±l deÄŸiÅŸtirebilirim',
    'Site Ã§ok yavaÅŸ yÃ¼kleniyor',
    'Personel Ã§ok kaba davrandÄ±'
]

labels = ['payment_issue', 'user_error', 'technical_issue', 'complaint']

# Feature extraction
extractor = FeatureExtractor()
X, feature_names = extractor.extract_tfidf_features(texts, max_features=100)

# Model eÄŸit
nb = NaiveBayesClassifier('multinomial')
nb.train(X, labels, feature_names)

# Test
test_text = ['Kredi kartÄ±mdan fazla para kesildi']
test_X = extractor.transform_new_text(test_text, 'tfidf')
prediction = nb.predict(test_X)
probabilities = nb.predict_proba(test_X)
```

**Beklenen Ã‡Ä±ktÄ±:**
```
ğŸ”¢ TF-IDF Ã¶zellikleri Ã§Ä±karÄ±lÄ±yor (max_features=100, ngram_range=(1, 2))
âœ… TF-IDF: 1 Ã¶zellik Ã§Ä±karÄ±ldÄ±
ğŸ¯ Naive Bayes (multinomial) eÄŸitimi baÅŸlÄ±yor...
âœ… EÄŸitim tamamlandÄ±! SÃ¼re: 0.00s
   SÄ±nÄ±f sayÄ±sÄ±: 4
   Ã–zellik sayÄ±sÄ±: 1

ğŸ¯ Test Metni: Kredi kartÄ±mdan fazla para kesildi
ğŸ“‹ Tahmin: payment_issue
ğŸ² OlasÄ±lÄ±klar:
   payment_issue: 0.400
   user_error: 0.200
   technical_issue: 0.200
   complaint: 0.200
```

---

## 5. Multinomial vs Gaussian

### ğŸ”¢ Multinomial Naive Bayes
- **KullanÄ±m:** Metin verisi, kelime sayÄ±larÄ±
- **Veri tipi:** Discrete (ayrÄ±k) deÄŸerler
- **Ã–rnek:** TF-IDF vektÃ¶rleri, kelime frekanslarÄ±
- **VarsayÄ±m:** Multinomial daÄŸÄ±lÄ±m

```python
# Ã–rnek veri
texts = ["para Ã¶deme", "ÅŸifre giriÅŸ"]
# TF-IDF â†’ [0.2, 0.8, 0.0, 0.6] gibi sayÄ±lar
```

### ğŸ“Š Gaussian Naive Bayes
- **KullanÄ±m:** SÃ¼rekli sayÄ±sal veriler
- **Veri tipi:** Continuous (sÃ¼rekli) deÄŸerler
- **Ã–rnek:** YaÅŸ, maaÅŸ, boyut Ã¶lÃ§Ã¼mleri
- **VarsayÄ±m:** Normal (Gaussian) daÄŸÄ±lÄ±m

```python
# Ã–rnek veri
features = [[25, 50000], [30, 60000]]  # [yaÅŸ, maaÅŸ]
# Normal daÄŸÄ±lÄ±m varsayÄ±mÄ±
```

### âš–ï¸ KarÅŸÄ±laÅŸtÄ±rma

| Ã–zellik | Multinomial | Gaussian |
|---------|-------------|----------|
| **Veri Tipi** | Discrete/Count | Continuous |
| **KullanÄ±m** | Metin, NLP | SayÄ±sal Ã¶zellikler |
| **HÄ±z** | Ã‡ok HÄ±zlÄ± | HÄ±zlÄ± |
| **Sparse Matrix** | Destekler | Desteklemez |
| **Smoothing** | Alpha parametresi | Var_smoothing |

---

## 6. GÃ¼Ã§lÃ¼ ve ZayÄ±f YanlarÄ±

### âœ… GÃ¼Ã§lÃ¼ YanlarÄ±

1. **SÃ¼per HÄ±zlÄ±**: Milisaniyeler iÃ§inde eÄŸitim
2. **Az Veri ile Ã‡alÄ±ÅŸÄ±r**: KÃ¼Ã§Ã¼k datasets iÃ§in ideal
3. **Basit ve AnlaÅŸÄ±lÄ±r**: Yorumlanabilir sonuÃ§lar
4. **Overfitting Riski DÃ¼ÅŸÃ¼k**: Robust model
5. **Ã–lÃ§eklenebilir**: BÃ¼yÃ¼k verilerde de hÄ±zlÄ±
6. **Baseline Model**: DiÄŸer algoritmalarÄ± deÄŸerlendirmek iÃ§in referans

### âŒ ZayÄ±f YanlarÄ±

1. **"Naive" VarsayÄ±m**: Ã–zellikler baÄŸÄ±msÄ±z (gerÃ§ekte deÄŸil)
   ```python
   # GerÃ§ek: "kredi kartÄ±" kelime Ã§ifti baÄŸÄ±mlÄ±
   # Naive Bayes: "kredi" ve "kartÄ±" baÄŸÄ±msÄ±z der
   ```

2. **Kelime SÄ±rasÄ± Ã–nemli DeÄŸil**: 
   ```python
   "Para Ã§ekildi" == "Ã‡ekildi para"  # AynÄ± kabul edilir
   ```

3. **KarmaÅŸÄ±k Ä°liÅŸkileri Yakalayamaz**: Non-linear patterns
4. **Feature Engineering Gerekli**: Ham metinle Ã§alÄ±ÅŸamaz

---

## 7. Hiperparametre Optimizasyonu

### ğŸ”§ Multinomial NB Parametreleri

```python
def hyperparameter_tuning(self, X_train, y_train, cv=5):
    param_grid = {
        'alpha': [0.01, 0.1, 0.5, 1.0, 2.0, 5.0],
        'fit_prior': [True, False]
    }
```

**Alpha (Smoothing Parameter):**
- **DÃ¼ÅŸÃ¼k alpha (0.01)**: Sert kararlar, seen data'ya odaklanÄ±r
- **YÃ¼ksek alpha (5.0)**: YumuÅŸak kararlar, unseen data'yÄ± tolere eder

**Fit_prior:**
- **True**: Prior olasÄ±lÄ±klarÄ± eÄŸitim verisinden Ã¶ÄŸren
- **False**: Uniform prior (tÃ¼m sÄ±nÄ±flar eÅŸit olasÄ±lÄ±k)

### ğŸ”¬ Gaussian NB Parametreleri

```python
param_grid = {
    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]
}
```

**Var_smoothing:**
- Varyans hesaplamalarÄ±na eklenen smoothing faktÃ¶rÃ¼
- Numerical stability iÃ§in gerekli

### ğŸ“Š Grid Search Ã–rneÄŸi

```python
# Hiperparametre optimizasyonu
best_params, best_score = nb.hyperparameter_tuning(X_train, y_train, cv=5)

# Beklenen Ã§Ä±ktÄ±:
# âœ… En iyi parametreler: {'alpha': 0.1, 'fit_prior': True}
# âœ… En iyi CV skoru: 0.8542
```

---

## 8. Kritik Noktalarda Dikkat Edilecekler

### ğŸš¨ Laplace Smoothing (Alpha)

**Problem:**
```python
# EÄŸer bir kelime hiÃ§ gÃ¶rÃ¼lmemiÅŸse
P("yenikelime" | kategori) = 0/total_words = 0
# 0 ile Ã§arpÄ±m = 0 (TÃ¼m hesap bozulur!)
```

**Ã‡Ã¶zÃ¼m:**
```python
P(kelime | kategori) = (count + alpha) / (total_words + alpha Ã— vocabulary_size)
# Alpha = 1 â†’ Laplace smoothing
# Alpha < 1 â†’ Lidstone smoothing
```

### ğŸ”¢ Log-Space Hesaplama

**Problem:**
```python
# Normal hesaplama (tehlikeli!)
likelihood = P(w1) Ã— P(w2) Ã— P(w3) Ã— ... Ã— P(wn)
# Ã‡ok kÃ¼Ã§Ã¼k sayÄ±lar â†’ Underflow!
```

**Ã‡Ã¶zÃ¼m:**
```python
# Log-space (gÃ¼venli!)
log_likelihood = log(P(w1)) + log(P(w2)) + ... + log(P(wn))
# Ã‡arpÄ±m â†’ Toplam
```

### ğŸ’¾ Model Persistence

```python
def save_model(self, filepath):
    model_data = {
        'model': self.model,
        'model_type': self.model_type,
        'feature_names': self.feature_names,
        'classes': self.classes,
        'is_trained': self.is_trained
    }
    joblib.dump(model_data, filepath)

def load_model(self, filepath):
    model_data = joblib.load(filepath)
    self.model = model_data['model']
    # ... diÄŸer bilgileri geri yÃ¼kle
```

---

## 9. Performans DeÄŸerlendirme

### ğŸ“ˆ Evaluation Metrikleri

```python
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Accuracy
accuracy = accuracy_score(y_test, y_pred)

# DetaylÄ± rapor
print(classification_report(y_test, y_pred, target_names=classes))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
```

**Ã–rnek Ã‡Ä±ktÄ±:**
```
                     precision    recall  f1-score   support

          complaint       0.85      0.82      0.83        63
       general_info       0.88      0.90      0.89        71
      payment_issue       0.92      0.89      0.90        61
reservation_problem       0.87      0.85      0.86        71
    technical_issue       0.83      0.87      0.85        70
         user_error       0.86      0.88      0.87        64

           accuracy                           0.87       400
          macro avg       0.87      0.87      0.87       400
       weighted avg       0.87      0.87      0.87       400
```

### â±ï¸ Performans Benchmarks

**EÄŸitilmiÅŸ model sonuÃ§larÄ± (gerÃ§ek proje):**
```
ğŸ¯ Naive Bayes (multinomial) eÄŸitimi baÅŸlÄ±yor...
âœ… EÄŸitim tamamlandÄ±! SÃ¼re: 0.004s
   SÄ±nÄ±f sayÄ±sÄ±: 6
   Ã–zellik sayÄ±sÄ±: 725

âœ… Test SonuÃ§larÄ±:
   Accuracy: 1.0000
   Tahmin sÃ¼resi: 0.0004s
```

---

## 10. Ã–zet ve SonuÃ§

### ğŸ¯ Ne Ã–ÄŸrendik?

1. **Bayes Teoremi**: P(A|B) = P(B|A) Ã— P(A) / P(B)
2. **Naive VarsayÄ±m**: Ã–zellikler baÄŸÄ±msÄ±z
3. **Ä°ki Ana Tip**: Multinomial (metin) vs Gaussian (sayÄ±sal)
4. **HÄ±z**: Ã‡ok hÄ±zlÄ± eÄŸitim ve tahmin
5. **Yorumlanabilirlik**: Feature importance analizi
6. **Hiperparametreler**: Alpha (smoothing), fit_prior

### ğŸš€ SÄ±radaki AdÄ±mlar

1. **Logistic Regression** ile karÅŸÄ±laÅŸtÄ±rma
2. **BERT** gibi deep learning modelleri
3. **Feature Engineering** teknikleri
4. **Cross-validation** ve model selection
5. **Production deployment** stratejileri

### ğŸ’¡ Pratik Tavsiyeler

1. **BaÅŸlangÄ±Ã§ modeli** olarak her zaman deneyin
2. **Baseline** performans iÃ§in kullanÄ±n
3. **HÄ±zlÄ± prototyping** iÃ§in ideal
4. **KÃ¼Ã§Ã¼k verilerle** mÃ¼kemmel Ã§alÄ±ÅŸÄ±r
5. **Feature importance** ile domain insight edinin

---

## ğŸ› ï¸ Matematik Pratikte Nerede KullanÄ±lÄ±yor?

**"Biz kodda bu matematiÄŸi nerede kullanÄ±yoruz ki?"** - En haklÄ± soru! 

### ğŸ¯ GerÃ§ek Senaryolar

#### 1ï¸âƒ£ **DEBUGGING - Model YanlÄ±ÅŸ Tahmin Ediyor**

**Problem:** Model "Ã¶deme ÅŸifre sorunu" metnini `user_error` olarak sÄ±nÄ±flandÄ±rÄ±yor ama bu `payment_issue` olmalÄ±.

**ğŸ” Matematik Biliyorsan:**
```python
# Log probabilities kontrol edebilirsin
print("payment iÃ§in:")
print(f"log P('Ã¶deme'|payment) = -2.095")
print(f"log P('ÅŸifre'|payment) = -2.779")  # âš ï¸ Bu Ã§ok dÃ¼ÅŸÃ¼k!

print("user_error iÃ§in:")  
print(f"log P('Ã¶deme'|user_error) = -2.350")
print(f"log P('ÅŸifre'|user_error) = -2.018")  # âœ… Bu daha yÃ¼ksek

# Ã‡Ã–ZÃœM: 'ÅŸifre' kelimesi payment kategorisinde az geÃ§iyor
# Daha fazla 'Ã¶deme ÅŸifre' iÃ§eren payment Ã¶rneÄŸi ekle!
```

**ğŸš« Matematik Bilmiyorsan:**
- "Model bozuk" deyip random hyperparameter denersin
- Grid search yapar ama nedenini anlamazsÄ±n
- Veri problemi olduÄŸunu fark edemezsin

#### 2ï¸âƒ£ **HIPERPARAMETRE TUNING**

**GerÃ§ek Ã‡Ä±ktÄ±:**
```python
# Alpha=0.01 (dÃ¼ÅŸÃ¼k smoothing)
P(payment|'para') = 0.892 (89.2%)  # Ã‡ok emin
P(user_error|'para') = 0.108 (10.8%)

# Alpha=5.0 (yÃ¼ksek smoothing)  
P(payment|'para') = 0.724 (72.4%)  # Daha temkinli
P(user_error|'para') = 0.276 (27.6%)
```

**ğŸ” Matematik Biliyorsan:**
```python
# FormÃ¼lÃ¼ biliyorsun:
# P(kelime|kategori) = (count + alpha) / (total + alpha Ã— V)

# Alpha dÃ¼ÅŸÃ¼k â†’ count dominant olur â†’ overfitting
# Alpha yÃ¼ksek â†’ smoothing dominant olur â†’ underfitting

# Veri boyutuna gÃ¶re optimum seÃ§ebilirsin
if len(training_data) < 1000:
    alpha = 1.0  # Orta seviye smoothing
else:
    alpha = 0.1  # Daha az smoothing
```

#### 3ï¸âƒ£ **FEATURE IMPORTANCE ANALIZI**

**GerÃ§ek Kod:**
```python
# Hangi kelimeler hangi kategori iÃ§in Ã¶nemli?
feature_log_prob = model.feature_log_prob_

for class_idx, class_name in enumerate(model.classes_):
    print(f"\n{class_name} iÃ§in en Ã¶nemli kelimeler:")
    for feature_idx in top_features[class_idx]:
        feature_name = feature_names[feature_idx]
        importance = feature_log_prob[class_idx][feature_idx]
        print(f"  {feature_name}: {importance:.4f}")
```

**Ã‡Ä±ktÄ±:**
```
payment_issue iÃ§in en Ã¶nemli kelimeler:
  para: -1.234  âœ… YÃ¼ksek olasÄ±lÄ±k  
  Ã¶deme: -1.456
  kart: -1.789

user_error iÃ§in en Ã¶nemli kelimeler:
  ÅŸifre: -1.123 âœ… YÃ¼ksek olasÄ±lÄ±k
  unuttum: -1.345
  giriÅŸ: -1.678
```

#### 4ï¸âƒ£ **PRODUCTION DEBUGGING**

**Senaryo:** Model production'da accuracy %90'dan %60'a dÃ¼ÅŸtÃ¼.

**ğŸ” Matematik Biliyorsan:**
```python
# Prior distributions deÄŸiÅŸti mi?
old_priors = [0.3, 0.2, 0.5]  # [payment, user_error, complaint]
new_priors = [0.1, 0.1, 0.8]  # âš ï¸ Complaint artmÄ±ÅŸ!

# Ã‡Ã¶zÃ¼m: Model'i yeni prior'larla retrain et
# Ya da class_weight parametresi kullan

model = MultinomialNB(fit_prior=True)  # Otomatik prior learning
```

#### 5ï¸âƒ£ **CONFIDENT PREDICTION**

**GerÃ§ek KullanÄ±m:**
```python
def get_confident_prediction(text, threshold=0.7):
    proba = model.predict_proba([text])[0]
    max_proba = max(proba)
    
    if max_proba >= threshold:
        prediction = model.predict([text])[0]
        return prediction, max_proba
    else:
        return "UNCERTAIN", max_proba

# Ã–rnek
result, confidence = get_confident_prediction("para problemi", 0.7)
if result == "UNCERTAIN":
    # Manuel review'e gÃ¶nder
    send_to_human_review(text)
```

### ğŸ§® Manuel vs Sklearn KarÅŸÄ±laÅŸtÄ±rma

**Test:** "para" kelimesi iÃ§in tahmin

**Manuel Hesaplama:**
```
P(payment|'para') = 0.738 (73.8%)
P(user_error|'para') = 0.262 (26.2%)
```

**Sklearn Sonucu:**
```
P(payment|'para') = 0.658 (65.8%)  
P(user_error|'para') = 0.342 (34.2%)
```

**Fark Neden Var?**
- Manuel hesaplama: Sadece kelime sayÄ±mÄ±
- Sklearn: TF-IDF aÄŸÄ±rlÄ±klandÄ±rma kullanÄ±yor
- Bu fark normal ve beklenen!

### ğŸ¯ Ã–zet: Matematik Nerede Gerekli?

| Durum | Matematik Biliyorsan | Bilmiyorsan |
|-------|---------------------|-------------|
| **Model Debug** | Root cause bulursun | Random deneme yanÄ±lma |
| **Hyperparameter** | BilinÃ§li seÃ§im | Grid search + umut |
| **Feature Selection** | Importance skorlarÄ± | Deneme yanÄ±lma |
| **Confidence** | Threshold belirleme | Accuracy'ye gÃ¼venme |
| **Production Issues** | Sistematik analiz | "Model bozuldu" |

**SonuÃ§:** Matematik, ML'de debugging tool'u! ğŸ› ï¸

---

## ğŸ“š Referanslar ve Ä°leri Okuma

- [Scikit-learn Naive Bayes Documentation](https://scikit-learn.org/stable/modules/naive_bayes.html)
- [Bayes' Theorem - Khan Academy](https://www.khanacademy.org/math/statistics-probability/probability-library/bayes-theorem/a/bayes-theorem-review)
- [Text Classification with Naive Bayes](https://web.stanford.edu/class/cs124/lec/naivebayes.pdf)
- [AutoTicket Classifier - GitHub Repository](https://github.com/your-repo/AutoTicketClassifier)

---

**ğŸ“ Notlar:**
- Bu dokÃ¼man AutoTicket Classifier projesi Ã¼zerinden hazÄ±rlanmÄ±ÅŸtÄ±r
- TÃ¼m kod Ã¶rnekleri Ã§alÄ±ÅŸÄ±r durumda test edilmiÅŸtir  
- SorularÄ±nÄ±z iÃ§in issue aÃ§abilir veya pull request gÃ¶nderebilirsiniz

**ğŸ·ï¸ Etiketler:** `machine-learning`, `naive-bayes`, `text-classification`, `nlp`, `python`, `scikit-learn`
