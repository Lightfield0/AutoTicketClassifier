# ğŸ¯ Naive Bayes Classifier - DetaylÄ± AnlatÄ±m ve Kod Analizi

## ğŸ“š Ä°Ã§indekiler
1. [GiriÅŸ ve Temel Kavramlar](#giriÅŸ-ve-temel-kavramlar)
2. [Bayes Teoremi](#bayes-teoremi)
3. [Naive Bayes MatematiÄŸi](#naive-bayes-matematiÄŸi)
4. [Kod YapÄ±sÄ± Analizi](#kod-yapÄ±sÄ±-analizi)
5. [Pratik Uygulama](#pratik-uygulama)
6. [Demo ve Ã‡Ä±ktÄ±lar](#demo-ve-Ã§Ä±ktÄ±lar)
7. [Avantajlar ve Dezavantajlar](#avantajlar-ve-dezavantajlar)

---

## ğŸ¯ GiriÅŸ ve Temel Kavramlar

Naive Bayes, makine Ã¶ÄŸrenmesinde en temel ve etkili sÄ±nÄ±flandÄ±rma algoritmalarÄ±ndan biridir. "Naive" (naif) kelimesi, algoritmanÄ±n Ã¶zelliklerin birbirinden baÄŸÄ±msÄ±z olduÄŸunu varsaymasÄ±ndan gelir.

### ğŸ” Temel Prensip
Naive Bayes, bir veri noktasÄ±nÄ±n hangi sÄ±nÄ±fa ait olduÄŸunu, o sÄ±nÄ±fÄ±n Ã¶nceki gÃ¶zlemlenme sÄ±klÄ±ÄŸÄ±na (prior probability) ve mevcut Ã¶zelliklerin o sÄ±nÄ±fta gÃ¶rÃ¼lme olasÄ±lÄ±ÄŸÄ±na (likelihood) dayanarak tahmin eder.

### ğŸ“Š KullanÄ±m AlanlarÄ±
- **Metin SÄ±nÄ±flandÄ±rma**: Spam filtresi, duygu analizi, dokÃ¼man kategorilendirme
- **TÄ±bbi TanÄ±**: Semptomlardan hastalÄ±k tahmini
- **MÃ¼ÅŸteri Segmentasyonu**: MÃ¼ÅŸteri davranÄ±ÅŸ tahmini
- **GerÃ§ek ZamanlÄ± Tahmin**: HÄ±zlÄ± karar verme sistemleri

---

## ğŸ§® Bayes Teoremi

Naive Bayes'in temeli Bayes Teoremi'dir:

```
P(A|B) = P(B|A) Ã— P(A) / P(B)
```

### ğŸ“ FormÃ¼l AÃ§Ä±klamasÄ±:
- **P(A|B)**: B gerÃ§ekleÅŸtiÄŸinde A'nÄ±n olasÄ±lÄ±ÄŸÄ± (Posterior)
- **P(B|A)**: A gerÃ§ekleÅŸtiÄŸinde B'nin olasÄ±lÄ±ÄŸÄ± (Likelihood)
- **P(A)**: A'nÄ±n Ã¶nceki olasÄ±lÄ±ÄŸÄ± (Prior)
- **P(B)**: B'nin olasÄ±lÄ±ÄŸÄ± (Evidence)

### ğŸ¯ SÄ±nÄ±flandÄ±rma Ä°Ã§in Uyarlama:
```
P(SÄ±nÄ±f|Ã–zellikler) = P(Ã–zellikler|SÄ±nÄ±f) Ã— P(SÄ±nÄ±f) / P(Ã–zellikler)
```

---

## ğŸ”¢ Naive Bayes MatematiÄŸi

### 1. Temel FormÃ¼l
Bir Ã¶rneÄŸin sÄ±nÄ±f `C` ye ait olma olasÄ±lÄ±ÄŸÄ±:

```
P(C|Xâ‚,Xâ‚‚,...,Xâ‚™) âˆ P(C) Ã— âˆáµ¢ P(Xáµ¢|C)
```

### 2. Multinomial Naive Bayes
Metin veriler iÃ§in kullanÄ±lÄ±r. Ã–zelliklerin sayÄ±m deÄŸerleri (kelime frekanslarÄ±) iÃ§in uygundur.

```
P(Xáµ¢|C) = (Náµ¢c + Î±) / (Nc + Î±Ã—V)
```

- **Náµ¢c**: SÄ±nÄ±f C'de Ã¶zellik i'nin sayÄ±sÄ±
- **Nc**: SÄ±nÄ±f C'deki toplam Ã¶zellik sayÄ±sÄ±
- **Î±**: Smoothing parametresi (genelde 1.0)
- **V**: Toplam Ã¶zellik sayÄ±sÄ±

### 3. Gaussian Naive Bayes
SÃ¼rekli deÄŸerler iÃ§in kullanÄ±lÄ±r. Ã–zelliklerin normal daÄŸÄ±lÄ±m gÃ¶sterdiÄŸini varsayar.

```
P(Xáµ¢|C) = (1/âˆš(2Ï€ÏƒÂ²c)) Ã— exp(-((xáµ¢-Î¼c)Â²)/(2ÏƒÂ²c))
```

---

## ğŸ’» Kod YapÄ±sÄ± Analizi

### ğŸ—ï¸ Class YapÄ±sÄ±
```python
class NaiveBayesClassifier:
    def __init__(self, model_type='multinomial'):
        """
        Model tÃ¼rÃ¼ seÃ§imi:
        - 'multinomial': Metin/sayÄ±m verileri iÃ§in
        - 'gaussian': SÃ¼rekli deÄŸerler iÃ§in
        """
```

### ğŸ¯ Ana Fonksiyonlar

#### 1. EÄŸitim Fonksiyonu
```python
def train(self, X_train, y_train, feature_names=None):
    """
    Model eÄŸitimi:
    1. Veri tipi kontrolÃ¼ (sparse/dense)
    2. Model fitting
    3. Metadata kaydetme
    """
```

**Kod Analizi:**
```python
# Veri tipini kontrol et
if hasattr(X_train, 'toarray'):  # Sparse matrix ise
    if self.model_type == 'gaussian':
        X_train = X_train.toarray()  # Gaussian iÃ§in dense gerekli

# Modeli eÄŸit
self.model.fit(X_train, y_train)
```

#### 2. Tahmin Fonksiyonu
```python
def predict(self, X_test):
    """
    SÄ±nÄ±f tahmini yapar
    En yÃ¼ksek olasÄ±lÄ±klÄ± sÄ±nÄ±fÄ± dÃ¶ndÃ¼rÃ¼r
    """
```

#### 3. OlasÄ±lÄ±k Tahmini
```python
def predict_proba(self, X_test):
    """
    Her sÄ±nÄ±f iÃ§in olasÄ±lÄ±k deÄŸerleri dÃ¶ndÃ¼rÃ¼r
    GÃ¼ven seviyesi iÃ§in kullanÄ±lÄ±r
    """
```

### ğŸ† Ã–zellik Ã–nem Analizi
```python
def get_feature_importance(self, top_n=10):
    """
    MultinomialNB iÃ§in log-probability deÄŸerlerini kullanÄ±r
    Her sÄ±nÄ±f iÃ§in en Ã¶nemli Ã¶zellikleri bulur
    """
    feature_log_prob = self.model.feature_log_prob_
    # Her sÄ±nÄ±f iÃ§in en yÃ¼ksek log-probability deÄŸerleri
```

---

## ğŸ§ª Pratik Uygulama

### Demo Kodu Ã‡alÄ±ÅŸtÄ±rma

```python
# Demo Ã§alÄ±ÅŸtÄ±rma
python -c "
from models.naive_bayes import demo_naive_bayes
demo_naive_bayes()
"
```

### Beklenen Ã‡Ä±ktÄ± Ã–rneÄŸi:
```
ğŸ§ª Naive Bayes Demo
==============================
1. Multinomial Naive Bayes:
ğŸ¯ Naive Bayes (multinomial) eÄŸitimi baÅŸlÄ±yor...
âœ… EÄŸitim tamamlandÄ±! SÃ¼re: 0.01s
   SÄ±nÄ±f sayÄ±sÄ±: 3
   Ã–zellik sayÄ±sÄ±: 100

ğŸ§ª Test seti Ã¼zerinde deÄŸerlendirme...
âœ… Test SonuÃ§larÄ±:
   Accuracy: 0.8400
   Tahmin sÃ¼resi: 0.0010s

ğŸ“‹ SÄ±nÄ±flandÄ±rma Raporu:
              precision    recall  f1-score   support
           0       0.86      0.82      0.84        67
           1       0.83      0.85      0.84        66
           2       0.83      0.85      0.84        67
    accuracy                           0.84       200
   macro avg       0.84      0.84      0.84       200
weighted avg       0.84      0.84      0.84       200

==================================================
2. Gaussian Naive Bayes:
ğŸ¯ Naive Bayes (gaussian) eÄŸitimi baÅŸlÄ±yor...
âœ… EÄŸitim tamamlandÄ±! SÃ¼re: 0.00s
   SÄ±nÄ±f sayÄ±sÄ±: 3
   Ã–zellik sayÄ±sÄ±: 100

ğŸ“Š KarÅŸÄ±laÅŸtÄ±rma:
Multinomial NB - Accuracy: 0.8400, SÃ¼re: 0.010s
Gaussian NB   - Accuracy: 0.8600, SÃ¼re: 0.003s
```

---

## ğŸ¯ GerÃ§ek Veri ile Test

### Ã–zellik Ã‡Ä±karma ile Birlikte KullanÄ±m

```python
from utils.feature_extraction import FeatureExtractor
from models.naive_bayes import NaiveBayesClassifier

# Ã–rnek metin verileri
texts = [
    "Bu Ã¼rÃ¼n Ã§ok kaliteli ve hÄ±zlÄ± geldi",
    "Kargo Ã§ok yavaÅŸ, memnun deÄŸilim", 
    "Fiyat performans aÃ§Ä±sÄ±ndan mÃ¼kemmel"
]
labels = ["pozitif", "negatif", "pozitif"]

# Feature extraction
extractor = FeatureExtractor()
X = extractor.extract_features(texts)

# Model eÄŸitimi
classifier = NaiveBayesClassifier()
classifier.train(X, labels, extractor.get_feature_names())
```

---

## ğŸ“Š Model Ä°Ã§ YapÄ±sÄ± Analizi

### Bayes Teoremi HesabÄ± Ã–rneÄŸi

Diyelim ki elimizde ÅŸu veriler var:
```
SÄ±nÄ±flar: ["pozitif", "negatif"]
Kelimeler: ["harika", "kÃ¶tÃ¼", "Ã¼rÃ¼n"]

EÄŸitim verileri:
- "harika Ã¼rÃ¼n" -> pozitif
- "kÃ¶tÃ¼ Ã¼rÃ¼n" -> negatif  
- "harika" -> pozitif
```

#### Manuel Hesaplama:
```python
# Prior probabilities
P(pozitif) = 2/3 = 0.67
P(negatif) = 1/3 = 0.33

# Likelihood hesabÄ± (Laplace smoothing Î±=1 ile)
# P("harika"|pozitif) = (2+1) / (3+3) = 3/6 = 0.5
# P("Ã¼rÃ¼n"|pozitif) = (1+1) / (3+3) = 2/6 = 0.33

# Yeni metin: "harika Ã¼rÃ¼n"
P(pozitif|"harika Ã¼rÃ¼n") âˆ P(pozitif) Ã— P("harika"|pozitif) Ã— P("Ã¼rÃ¼n"|pozitif)
                         âˆ 0.67 Ã— 0.5 Ã— 0.33 = 0.11055
```

### Kod ile DoÄŸrulama:
```python
# Model iÃ§indeki deÄŸerlere eriÅŸim
print("Feature log probabilities:")
print(classifier.model.feature_log_prob_)

print("Class log priors:")
print(classifier.model.class_log_prior_)
```

---

## ğŸš€ Performans Optimizasyonu

### Hiperparametre Tuning

```python
def hyperparameter_tuning(self, X_train, y_train, cv=5):
    """
    Grid Search ile en iyi parametreleri bul
    """
    if self.model_type == 'multinomial':
        param_grid = {
            'alpha': [0.01, 0.1, 0.5, 1.0, 2.0, 5.0],  # Smoothing
            'fit_prior': [True, False]  # Prior kullanÄ±m
        }
    else:  # gaussian
        param_grid = {
            'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]
        }
```

### Beklenen Tuning Ã‡Ä±ktÄ±sÄ±:
```
ğŸ”§ Hiperparametre optimizasyonu baÅŸlÄ±yor...
âœ… En iyi parametreler: {'alpha': 0.1, 'fit_prior': True}
âœ… En iyi CV skoru: 0.8750
```

---

## ğŸ“ˆ Avantajlar ve Dezavantajlar

### âœ… Avantajlar:
1. **HÄ±z**: Ã‡ok hÄ±zlÄ± eÄŸitim ve tahmin
2. **Basitlik**: AnlaÅŸÄ±lmasÄ± ve uygulanmasÄ± kolay
3. **Az Veri**: KÃ¼Ã§Ã¼k veri setlerinde iyi Ã§alÄ±ÅŸÄ±r
4. **Ã–lÃ§eklenebilirlik**: BÃ¼yÃ¼k veri setlerinde verimli
5. **Multiclass**: DoÄŸal olarak Ã§ok sÄ±nÄ±flÄ±
6. **Baseline**: Ä°yi bir baÅŸlangÄ±Ã§ modeli

### âŒ Dezavantajlar:
1. **BaÄŸÄ±msÄ±zlÄ±k VarsayÄ±mÄ±**: Ã–zelliklerin baÄŸÄ±msÄ±z olduÄŸunu varsayar
2. **Kategorik Veriler**: SÃ¼rekli verilerle sÄ±nÄ±rlÄ± performans
3. **Zero Probability**: GÃ¶rÃ¼lmemiÅŸ kombinasyonlar iÃ§in smoothing gerekli
4. **Feature Engineering**: Ä°yi Ã¶zellik seÃ§imi kritik

---

## ğŸ¯ GerÃ§ek Proje Entegrasyonu

### AutoTicketClassifier Projemizdeki KullanÄ±m:

```python
# train_models.py'den
def train_naive_bayes_model(X_train, y_train, X_test, y_test):
    """
    Ticket sÄ±nÄ±flandÄ±rma iÃ§in Naive Bayes eÄŸitimi
    """
    print("ğŸ¯ Naive Bayes modeli eÄŸitiliyor...")
    
    model = MultinomialNB(alpha=1.0)
    model.fit(X_train, y_train)
    
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"âœ… Naive Bayes Accuracy: {accuracy:.4f}")
    return model, accuracy
```

### Web App Entegrasyonu:
```python
# web/app.py'de kullanÄ±m
@st.cache_resource
def load_models():
    """Modelleri yÃ¼kle"""
    models = {}
    
    try:
        models['naive_bayes'] = joblib.load('models/trained/naive_bayes_model.joblib')
        st.success("âœ… Naive Bayes modeli yÃ¼klendi")
    except Exception as e:
        st.error(f"âŒ Naive Bayes yÃ¼klenirken hata: {e}")
    
    return models
```

---

## ğŸ” Debugging ve Sorun Giderme

### YaygÄ±n Hatalar ve Ã‡Ã¶zÃ¼mleri:

#### 1. Sparse Matrix HatasÄ±
```python
# Hata: Gaussian NB sparse matrix kabul etmez
if hasattr(X_train, 'toarray') and self.model_type == 'gaussian':
    X_train = X_train.toarray()
```

#### 2. Zero Probability Problemi
```python
# Ã‡Ã¶zÃ¼m: Laplace smoothing
MultinomialNB(alpha=1.0)  # alpha > 0 olmalÄ±
```

#### 3. Feature Names UyumsuzluÄŸu
```python
# Model kaydetme sÄ±rasÄ±nda feature names'i de kaydet
model_data = {
    'model': self.model,
    'feature_names': self.feature_names,
    'classes': self.classes
}
```

---

## ğŸ“š Ek Kaynaklar ve Ä°leri Ã‡alÄ±ÅŸma

### ğŸ“– Ã–nerilen Okuma:
1. "Pattern Recognition and Machine Learning" - Christopher Bishop
2. "The Elements of Statistical Learning" - Hastie, Tibshirani, Friedman
3. Scikit-learn Documentation: Naive Bayes

### ğŸ”¬ Ä°leri Konular:
1. **Complement Naive Bayes**: Dengesiz veri setleri iÃ§in
2. **Bernoulli Naive Bayes**: Binary features iÃ§in
3. **Feature Selection**: Chi-square, Mutual Information
4. **Ensemble Methods**: Naive Bayes + diÄŸer algoritmalar

### ğŸ› ï¸ Pratik Projeler:
1. Spam e-posta filtreleme
2. Haber kategorilendirme
3. Duygu analizi sistemi
4. DokÃ¼man sÄ±nÄ±flandÄ±rma

---

## ğŸ SonuÃ§

Naive Bayes, basitliÄŸi ve etkinliÄŸi ile makine Ã¶ÄŸrenmesinin temel taÅŸlarÄ±ndan biridir. Bu anlatÄ±mda:

âœ… **Matematiksel temelleri** Ã¶ÄŸrendik  
âœ… **Kod implementasyonu** analiz ettik  
âœ… **Pratik uygulamalarÄ±** gÃ¶rdÃ¼k  
âœ… **Debugging teknikleri** Ã¶ÄŸrendik  
âœ… **GerÃ§ek proje entegrasyonu** yaptÄ±k  

Naive Bayes, Ã¶zellikle metin sÄ±nÄ±flandÄ±rma problemlerinde hala Ã§ok etkili bir algoritma. Modern deep learning yaklaÅŸÄ±mlarÄ±nÄ±n yanÄ±nda, hÄ±z ve basitlik gerektiren durumlarda tercih edilmeye devam ediyor.

---

*Bu dokÃ¼man, AutoTicketClassifier projesi kapsamÄ±nda hazÄ±rlanmÄ±ÅŸtÄ±r. SorularÄ±nÄ±z iÃ§in GitHub Issues kullanabilirsiniz.*

---

## ğŸ“ Notlar

- **Tarih**: 25 Haziran 2025
- **Versiyon**: 1.0
- **Proje**: AutoTicketClassifier
- **Yazar**: GitHub Copilot ile hazÄ±rlandÄ±
