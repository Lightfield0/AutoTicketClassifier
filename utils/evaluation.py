"""
ğŸ“ˆ Model Evaluation Utilities
Model performansÄ±nÄ± deÄŸerlendirme araÃ§larÄ±
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix, roc_auc_score,
    precision_recall_curve, roc_curve
)
from sklearn.model_selection import cross_val_score
import time

class ModelEvaluator:
    def __init__(self):
        self.results = {}
        
    def evaluate_model(self, model, X_test, y_test, y_pred=None, 
                      model_name="Model", class_names=None):
        """KapsamlÄ± model deÄŸerlendirmesi"""
        print(f"ğŸ“Š {model_name} deÄŸerlendiriliyor...")
        
        start_time = time.time()
        
        # Prediction varsa kullan, yoksa tahmin yap
        if y_pred is None:
            y_pred = model.predict(X_test)
        
        prediction_time = time.time() - start_time
        
        # Temel metrikler
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted')
        recall = recall_score(y_test, y_pred, average='weighted')
        f1 = f1_score(y_test, y_pred, average='weighted')
        
        # SonuÃ§larÄ± kaydet
        self.results[model_name] = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'prediction_time': prediction_time,
            'y_true': y_test,
            'y_pred': y_pred
        }
        
        print(f"âœ… {model_name} SonuÃ§larÄ±:")
        print(f"   Accuracy:  {accuracy:.4f}")
        print(f"   Precision: {precision:.4f}")
        print(f"   Recall:    {recall:.4f}")
        print(f"   F1-Score:  {f1:.4f}")
        print(f"   Tahmin SÃ¼resi: {prediction_time:.3f}s")
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'prediction_time': prediction_time
        }
    
    def print_classification_report(self, model_name, class_names=None):
        """DetaylÄ± sÄ±nÄ±flandÄ±rma raporu"""
        if model_name not in self.results:
            print(f"âŒ {model_name} iÃ§in sonuÃ§ bulunamadÄ±")
            return
        
        y_true = self.results[model_name]['y_true']
        y_pred = self.results[model_name]['y_pred']
        
        print(f"\nğŸ“‹ {model_name} - DetaylÄ± SÄ±nÄ±flandÄ±rma Raporu")
        print("=" * 60)
        
        report = classification_report(
            y_true, y_pred, 
            target_names=class_names,
            digits=4
        )
        print(report)
    
    def plot_confusion_matrix(self, model_name, class_names=None, figsize=(8, 6)):
        """Confusion matrix gÃ¶rselleÅŸtirme"""
        if model_name not in self.results:
            print(f"âŒ {model_name} iÃ§in sonuÃ§ bulunamadÄ±")
            return
        
        y_true = self.results[model_name]['y_true']
        y_pred = self.results[model_name]['y_pred']
        
        # Confusion matrix hesapla
        cm = confusion_matrix(y_true, y_pred)
        
        # Normalize et (yÃ¼zde olarak)
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        
        # Plot
        plt.figure(figsize=figsize)
        sns.heatmap(cm_normalized, 
                   annot=True, 
                   fmt='.2%',
                   cmap='Blues',
                   xticklabels=class_names or range(len(cm)),
                   yticklabels=class_names or range(len(cm)))
        
        plt.title(f'{model_name} - Confusion Matrix (%)')
        plt.ylabel('GerÃ§ek Kategori')
        plt.xlabel('Tahmin Edilen Kategori')
        plt.tight_layout()
        plt.show()
        
        return cm, cm_normalized
    
    def compare_models(self, figsize=(12, 8)):
        """Model karÅŸÄ±laÅŸtÄ±rmasÄ± gÃ¶rselleÅŸtirme"""
        if len(self.results) < 2:
            print("âŒ KarÅŸÄ±laÅŸtÄ±rma iÃ§in en az 2 model gerekli")
            return
        
        # SonuÃ§larÄ± DataFrame'e Ã§evir
        comparison_data = []
        for model_name, results in self.results.items():
            comparison_data.append({
                'Model': model_name,
                'Accuracy': results['accuracy'],
                'Precision': results['precision'],
                'Recall': results['recall'],
                'F1-Score': results['f1_score'],
                'Prediction Time (s)': results['prediction_time']
            })
        
        df_comparison = pd.DataFrame(comparison_data)
        
        # GÃ¶rselleÅŸtirme
        fig, axes = plt.subplots(2, 2, figsize=figsize)
        
        metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
        colors = ['skyblue', 'lightgreen', 'lightcoral', 'lightsalmon']
        
        for i, metric in enumerate(metrics):
            ax = axes[i//2, i%2]
            bars = ax.bar(df_comparison['Model'], df_comparison[metric], 
                         color=colors[i], alpha=0.7)
            ax.set_title(f'{metric} KarÅŸÄ±laÅŸtÄ±rmasÄ±')
            ax.set_ylim(0, 1)
            ax.tick_params(axis='x', rotation=45)
            
            # DeÄŸerleri bar'larÄ±n Ã¼zerine yaz
            for bar, value in zip(bars, df_comparison[metric]):
                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                       f'{value:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.show()
        
        return df_comparison
    
    def plot_performance_vs_time(self):
        """Performans vs HÄ±z karÅŸÄ±laÅŸtÄ±rmasÄ±"""
        if len(self.results) < 2:
            print("âŒ KarÅŸÄ±laÅŸtÄ±rma iÃ§in en az 2 model gerekli")
            return
        
        models = list(self.results.keys())
        f1_scores = [self.results[model]['f1_score'] for model in models]
        times = [self.results[model]['prediction_time'] for model in models]
        
        plt.figure(figsize=(10, 6))
        
        # Scatter plot
        scatter = plt.scatter(times, f1_scores, s=100, alpha=0.7, c=range(len(models)), cmap='viridis')
        
        # Model isimlerini ekle
        for i, model in enumerate(models):
            plt.annotate(model, (times[i], f1_scores[i]), 
                        xytext=(5, 5), textcoords='offset points')
        
        plt.xlabel('Tahmin SÃ¼resi (saniye)')
        plt.ylabel('F1-Score')
        plt.title('Model PerformansÄ± vs HÄ±z')
        plt.grid(True, alpha=0.3)
        
        # Ä°deal bÃ¶lge (dÃ¼ÅŸÃ¼k sÃ¼re, yÃ¼ksek performans)
        plt.axhline(y=max(f1_scores), color='green', linestyle='--', alpha=0.5, label='En Ä°yi F1-Score')
        plt.axvline(x=min(times), color='blue', linestyle='--', alpha=0.5, label='En HÄ±zlÄ± Model')
        
        plt.legend()
        plt.tight_layout()
        plt.show()
    
    def cross_validate_model(self, model, X, y, cv=5, scoring='f1_weighted'):
        """Cross-validation ile model deÄŸerlendirme"""
        print(f"ğŸ”„ {cv}-fold cross-validation yapÄ±lÄ±yor...")
        
        start_time = time.time()
        scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)
        cv_time = time.time() - start_time
        
        print(f"âœ… Cross-Validation SonuÃ§larÄ± ({scoring}):")
        print(f"   Ortalama: {scores.mean():.4f} (Â±{scores.std() * 2:.4f})")
        print(f"   Min: {scores.min():.4f}")
        print(f"   Max: {scores.max():.4f}")
        print(f"   CV SÃ¼resi: {cv_time:.2f}s")
        
        return scores
    
    def generate_summary_report(self, class_names=None):
        """Ã–zet rapor oluÅŸtur"""
        if not self.results:
            print("âŒ DeÄŸerlendirilecek model bulunamadÄ±")
            return
        
        print("\n" + "="*80)
        print("ğŸ“Š MODEL PERFORMANS Ã–ZET RAPORU")
        print("="*80)
        
        # En iyi modeli bul
        best_model = max(self.results.keys(), 
                        key=lambda x: self.results[x]['f1_score'])
        
        print(f"\nğŸ† En Ä°yi Model: {best_model}")
        print(f"   F1-Score: {self.results[best_model]['f1_score']:.4f}")
        print(f"   Accuracy: {self.results[best_model]['accuracy']:.4f}")
        
        # En hÄ±zlÄ± model
        fastest_model = min(self.results.keys(),
                           key=lambda x: self.results[x]['prediction_time'])
        
        print(f"\nâš¡ En HÄ±zlÄ± Model: {fastest_model}")
        print(f"   Tahmin SÃ¼resi: {self.results[fastest_model]['prediction_time']:.3f}s")
        print(f"   F1-Score: {self.results[fastest_model]['f1_score']:.4f}")
        
        # TÃ¼m modeller tablosu
        print(f"\nğŸ“‹ TÃ¼m Modeller:")
        print(f"{'Model':<20} {'Accuracy':<10} {'F1-Score':<10} {'SÃ¼re (s)':<10}")
        print("-" * 50)
        
        for model_name, results in self.results.items():
            print(f"{model_name:<20} {results['accuracy']:<10.4f} {results['f1_score']:<10.4f} {results['prediction_time']:<10.3f}")
        
        # Ã–neriler
        print(f"\nğŸ’¡ Ã–neriler:")
        if self.results[best_model]['f1_score'] > 0.9:
            print("   âœ… MÃ¼kemmel performans! Production'a hazÄ±r.")
        elif self.results[best_model]['f1_score'] > 0.8:
            print("   âœ… Ä°yi performans! KÃ¼Ã§Ã¼k iyileÅŸtirmeler yapÄ±labilir.")
        else:
            print("   âš ï¸  Performans artÄ±rÄ±mÄ± gerekli. Daha fazla veri veya feature engineering Ã¶nerilidir.")
        
        if self.results[fastest_model]['prediction_time'] < 0.1:
            print("   âš¡ HÄ±z mÃ¼kemmel! Real-time uygulamalar iÃ§in uygun.")
        elif self.results[fastest_model]['prediction_time'] < 1.0:
            print("   âš¡ HÄ±z iyi! Ã‡oÄŸu uygulama iÃ§in yeterli.")
        else:
            print("   ğŸŒ HÄ±z optimizasyonu gerekebilir.")

def demo_evaluation():
    """DeÄŸerlendirme demo'su"""
    print("ğŸ§ª Model DeÄŸerlendirme Demo'su")
    print("=" * 50)
    
    # Ã–rnek veri oluÅŸtur
    np.random.seed(42)
    n_samples = 1000
    n_classes = 6
    
    y_true = np.random.randint(0, n_classes, n_samples)
    
    # FarklÄ± performanslarda 3 model simÃ¼lasyonu
    # Model 1: Ä°yi performans
    y_pred1 = y_true.copy()
    noise_indices = np.random.choice(n_samples, int(n_samples * 0.15), replace=False)
    y_pred1[noise_indices] = np.random.randint(0, n_classes, len(noise_indices))
    
    # Model 2: Orta performans
    y_pred2 = y_true.copy()
    noise_indices = np.random.choice(n_samples, int(n_samples * 0.25), replace=False)
    y_pred2[noise_indices] = np.random.randint(0, n_classes, len(noise_indices))
    
    # Model 3: DÃ¼ÅŸÃ¼k performans
    y_pred3 = y_true.copy()
    noise_indices = np.random.choice(n_samples, int(n_samples * 0.4), replace=False)
    y_pred3[noise_indices] = np.random.randint(0, n_classes, len(noise_indices))
    
    # Evaluator oluÅŸtur
    evaluator = ModelEvaluator()
    
    # Modelleri deÄŸerlendir
    class MockModel:
        def __init__(self, predictions):
            self.predictions = predictions
        def predict(self, X):
            return self.predictions
    
    X_dummy = np.zeros((n_samples, 10))  # Dummy features
    
    evaluator.evaluate_model(MockModel(y_pred1), X_dummy, y_true, model_name="BERT Classifier")
    evaluator.evaluate_model(MockModel(y_pred2), X_dummy, y_true, model_name="Logistic Regression")
    evaluator.evaluate_model(MockModel(y_pred3), X_dummy, y_true, model_name="Naive Bayes")
    
    # KarÅŸÄ±laÅŸtÄ±rma
    class_names = ["Ã–deme", "Rezervasyon", "KullanÄ±cÄ±", "Åikayet", "Bilgi", "Teknik"]
    comparison_df = evaluator.compare_models()
    
    # Ã–zet rapor
    evaluator.generate_summary_report(class_names)

if __name__ == "__main__":
    demo_evaluation()
