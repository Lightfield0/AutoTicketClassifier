"""
ğŸ”§ Text Preprocessing Utilities
TÃ¼rkÃ§e metin Ã¶n iÅŸleme araÃ§larÄ±
"""

import re
import string
import nltk
import warnings
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import SnowballStemmer
import pandas as pd

# Suppress warnings for Turkish stemmer
warnings.filterwarnings('ignore', message='.*Turkish.*stemmer.*')

class TurkishTextPreprocessor:
    def __init__(self):
        # NLTK verilerini indir
        try:
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            nltk.download('punkt')
            
        try:
            nltk.data.find('corpora/stopwords')
        except LookupError:
            nltk.download('stopwords')
        
        # TÃ¼rkÃ§e stop words
        self.stop_words = set(stopwords.words('turkish'))
        
        # Ek TÃ¼rkÃ§e stop words
        turkish_custom_stops = {
            'bir', 'bu', 'ÅŸu', 'o', 've', 'ile', 'iÃ§in', 'da', 'de', 'ta', 'te',
            'ki', 'mi', 'mÄ±', 'mu', 'mÃ¼', 'var', 'yok', 'gibi', 'kadar', 'daha',
            'en', 'Ã§ok', 'az', 'biraz', 'hiÃ§', 'her', 'bazÄ±', 'ÅŸey', 'kez',
            'zaman', 'yer', 'nasÄ±l', 'neden', 'niÃ§in', 'niye', 'ne', 'hangi',
            'kim', 'kimi', 'kimin', 'kimse', 'hiÃ§bir', 'hiÃ§birÅŸey'
        }
        self.stop_words.update(turkish_custom_stops)
        
        # TÃ¼rkÃ§e stemmer - uyarÄ±larÄ± bastÄ±r
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                self.stemmer = SnowballStemmer('turkish')
        except (ValueError, LookupError) as e:
            # TÃ¼rkÃ§e desteklenmiyorsa stemmer kullanma
            self.stemmer = None
            print("âš ï¸ TÃ¼rkÃ§e stemmer desteklenmiyor, stemming atlanacak")
        
        # TÃ¼rkÃ§e karakter normalizasyonu
        self.char_map = {
            'Ã§': 'c', 'ÄŸ': 'g', 'Ä±': 'i', 'Ã¶': 'o', 'ÅŸ': 's', 'Ã¼': 'u',
            'Ã‡': 'C', 'Ä': 'G', 'I': 'I', 'Ä°': 'I', 'Ã–': 'O', 'Å': 'S', 'Ãœ': 'U'
        }

    def normalize_turkish_chars(self, text, remove_turkish=False):
        """TÃ¼rkÃ§e karakterleri normalize eder"""
        if remove_turkish:
            # TÃ¼rkÃ§e karakterleri Ä°ngilizce karÅŸÄ±lÄ±klarÄ±yla deÄŸiÅŸtir
            for turkish_char, english_char in self.char_map.items():
                text = text.replace(turkish_char, english_char)
        return text

    def clean_text(self, text):
        """Temel metin temizliÄŸi"""
        if pd.isna(text):
            return ""
            
        # KÃ¼Ã§Ã¼k harfe Ã§evir
        text = str(text).lower()
        
        # Email adresleri temizle
        text = re.sub(r'\S+@\S+', '', text)
        
        # URL'leri temizle
        text = re.sub(r'http\S+|www\S+', '', text)
        
        # Telefon numaralarÄ±nÄ± temizle
        text = re.sub(r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b', '', text)
        text = re.sub(r'\b0\d{3}\s?\d{3}\s?\d{2}\s?\d{2}\b', '', text)
        
        # Ã–zel karakterleri temizle (TÃ¼rkÃ§e karakterleri koru)
        text = re.sub(r'[^\w\sÃ§ÄŸÄ±Ã¶ÅŸÃ¼Ã‡ÄIÄ°Ã–ÅÃœ]', ' ', text)
        
        # Ã‡oklu boÅŸluklarÄ± tek boÅŸluÄŸa Ã§evir
        text = re.sub(r'\s+', ' ', text)
        
        # BaÅŸta ve sonda boÅŸluk temizle
        text = text.strip()
        
        return text

    def remove_stopwords(self, text):
        """Stop words'leri kaldÄ±rÄ±r"""
        words = word_tokenize(text)
        filtered_words = [word for word in words if word.lower() not in self.stop_words]
        return ' '.join(filtered_words)

    def stem_text(self, text):
        """Kelimeleri kÃ¶kenlerine indirgemer"""
        if not self.stemmer:
            # Stemmer yoksa orijinal metni dÃ¶ndÃ¼r
            return text
            
        words = word_tokenize(text)
        stemmed_words = [self.stemmer.stem(word) for word in words]
        return ' '.join(stemmed_words)

    def preprocess_text(self, text, 
                       remove_stopwords=True, 
                       apply_stemming=False,
                       normalize_turkish=False,
                       min_length=2):
        """Tam metin Ã¶n iÅŸleme pipeline'Ä±"""
        
        # Temel temizlik
        text = self.clean_text(text)
        
        if not text:
            return ""
        
        # TÃ¼rkÃ§e karakter normalizasyonu
        if normalize_turkish:
            text = self.normalize_turkish_chars(text, remove_turkish=True)
        
        # Stop words kaldÄ±r
        if remove_stopwords:
            text = self.remove_stopwords(text)
        
        # Stemming uygula
        if apply_stemming:
            text = self.stem_text(text)
        
        # Minimum uzunluk kontrolÃ¼
        words = text.split()
        words = [word for word in words if len(word) >= min_length]
        text = ' '.join(words)
        
        return text

    def preprocess_dataframe(self, df, text_column='message', 
                           new_column='processed_text', **kwargs):
        """DataFrame'deki metinleri Ã¶n iÅŸler"""
        print("ğŸ”§ Metin Ã¶n iÅŸleme baÅŸlÄ±yor...")
        
        df[new_column] = df[text_column].apply(
            lambda x: self.preprocess_text(x, **kwargs)
        )
        
        # Ä°statistikler
        original_lengths = df[text_column].str.len()
        processed_lengths = df[new_column].str.len()
        
        print(f"ğŸ“Š Ã–n Ä°ÅŸleme SonuÃ§larÄ±:")
        print(f"   Ortalama uzunluk: {original_lengths.mean():.1f} â†’ {processed_lengths.mean():.1f}")
        print(f"   Medyan uzunluk: {original_lengths.median():.1f} â†’ {processed_lengths.median():.1f}")
        
        # BoÅŸ metinleri kontrol et
        empty_count = (df[new_column].str.len() == 0).sum()
        if empty_count > 0:
            print(f"âš ï¸  {empty_count} adet boÅŸ metin bulundu")
            # BoÅŸ metinleri orijinal metinle deÄŸiÅŸtir
            df.loc[df[new_column].str.len() == 0, new_column] = df.loc[df[new_column].str.len() == 0, text_column]
        
        print("âœ… Metin Ã¶n iÅŸleme tamamlandÄ±")
        return df

# Kolay kullanÄ±m iÃ§in fonksiyonlar
def quick_clean(text):
    """HÄ±zlÄ± metin temizliÄŸi"""
    preprocessor = TurkishTextPreprocessor()
    return preprocessor.preprocess_text(text, remove_stopwords=True, apply_stemming=False)

def deep_clean(text):
    """Derin metin temizliÄŸi (stemming dahil)"""
    preprocessor = TurkishTextPreprocessor()
    return preprocessor.preprocess_text(text, remove_stopwords=True, apply_stemming=True)

def english_compatible_clean(text):
    """Ä°ngilizce uyumlu temizlik (TÃ¼rkÃ§e karakterler Ã§evrilir)"""
    preprocessor = TurkishTextPreprocessor()
    return preprocessor.preprocess_text(text, 
                                      remove_stopwords=True, 
                                      apply_stemming=False,
                                      normalize_turkish=True)

if __name__ == "__main__":
    # Test
    preprocessor = TurkishTextPreprocessor()
    
    test_texts = [
        "Merhaba, kredi kartÄ±mdan para Ã§ekildi ama rezervasyonum onaylanmadÄ±!",
        "Åifremi unuttum, nasÄ±l deÄŸiÅŸtirebilirim? Ã‡ok acil...",
        "Site Ã§ok yavaÅŸ yÃ¼kleniyor. Bu durumda Ã§ok Ã¼zgÃ¼nÃ¼m.",
        "Ä°yi gÃ¼nler, Ã§alÄ±ÅŸma saatleriniz nedir?"
    ]
    
    print("ğŸ§ª Metin Ã–n Ä°ÅŸleme Testi")
    print("=" * 50)
    
    for i, text in enumerate(test_texts, 1):
        cleaned = preprocessor.preprocess_text(text)
        print(f"\n{i}. Orijinal: {text}")
        print(f"   TemizlenmiÅŸ: {cleaned}")
