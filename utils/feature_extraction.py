"""
ğŸ¯ Feature Extraction Utilities
Metinlerden Ã¶zellik Ã§Ä±karma araÃ§larÄ±
"""

import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.feature_extraction.text import HashingVectorizer
from collections import Counter
import re

class FeatureExtractor:
    def __init__(self):
        self.tfidf_vectorizer = None
        self.count_vectorizer = None
        self.hashing_vectorizer = None
        
    def extract_tfidf_features(self, texts, max_features=5000, ngram_range=(1, 2)):
        """TF-IDF Ã¶zelliklerini Ã§Ä±karÄ±r"""
        print(f"ğŸ”¢ TF-IDF Ã¶zellikleri Ã§Ä±karÄ±lÄ±yor (max_features={max_features}, ngram_range={ngram_range})")
        
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=max_features,
            ngram_range=ngram_range,
            min_df=2,  # En az 2 dokÃ¼manda geÃ§meli
            max_df=0.95,  # %95'ten fazla dokÃ¼manda geÃ§mesin
            sublinear_tf=True,
            strip_accents='unicode'
        )
        
        tfidf_matrix = self.tfidf_vectorizer.fit_transform(texts)
        feature_names = self.tfidf_vectorizer.get_feature_names_out()
        
        print(f"âœ… TF-IDF: {tfidf_matrix.shape[1]} Ã¶zellik Ã§Ä±karÄ±ldÄ±")
        return tfidf_matrix, feature_names
    
    def extract_count_features(self, texts, max_features=5000, ngram_range=(1, 2)):
        """Count (Bag of Words) Ã¶zelliklerini Ã§Ä±karÄ±r"""
        print(f"ğŸ”¢ Count Ã¶zellikleri Ã§Ä±karÄ±lÄ±yor (max_features={max_features})")
        
        self.count_vectorizer = CountVectorizer(
            max_features=max_features,
            ngram_range=ngram_range,
            min_df=2,
            max_df=0.95,
            strip_accents='unicode'
        )
        
        count_matrix = self.count_vectorizer.fit_transform(texts)
        feature_names = self.count_vectorizer.get_feature_names_out()
        
        print(f"âœ… Count: {count_matrix.shape[1]} Ã¶zellik Ã§Ä±karÄ±ldÄ±")
        return count_matrix, feature_names
    
    def extract_hashing_features(self, texts, n_features=10000):
        """Hashing Trick Ã¶zelliklerini Ã§Ä±karÄ±r (bÃ¼yÃ¼k veri iÃ§in)"""
        print(f"ğŸ”¢ Hashing Ã¶zellikleri Ã§Ä±karÄ±lÄ±yor (n_features={n_features})")
        
        self.hashing_vectorizer = HashingVectorizer(
            n_features=n_features,
            ngram_range=(1, 2),
            strip_accents='unicode'
        )
        
        hashing_matrix = self.hashing_vectorizer.fit_transform(texts)
        
        print(f"âœ… Hashing: {hashing_matrix.shape[1]} Ã¶zellik Ã§Ä±karÄ±ldÄ±")
        return hashing_matrix
    
    def extract_statistical_features(self, texts):
        """Ä°statistiksel Ã¶zellikler Ã§Ä±karÄ±r"""
        print("ğŸ“Š Ä°statistiksel Ã¶zellikler Ã§Ä±karÄ±lÄ±yor...")
        
        features = []
        
        for text in texts:
            feature_dict = {}
            
            # Temel uzunluk Ã¶zellikleri
            feature_dict['char_count'] = len(text)
            feature_dict['word_count'] = len(text.split())
            feature_dict['sentence_count'] = len(re.split(r'[.!?]+', text))
            feature_dict['avg_word_length'] = np.mean([len(word) for word in text.split()]) if text.split() else 0
            
            # Noktalama iÅŸaretleri
            feature_dict['exclamation_count'] = text.count('!')
            feature_dict['question_count'] = text.count('?')
            feature_dict['comma_count'] = text.count(',')
            feature_dict['period_count'] = text.count('.')
            
            # BÃ¼yÃ¼k harf kullanÄ±mÄ±
            feature_dict['uppercase_ratio'] = sum(1 for c in text if c.isupper()) / len(text) if text else 0
            feature_dict['title_case_words'] = sum(1 for word in text.split() if word.istitle())
            
            # SayÄ±sal Ã¶zellikler
            feature_dict['digit_count'] = sum(1 for c in text if c.isdigit())
            feature_dict['has_numbers'] = 1 if any(c.isdigit() for c in text) else 0
            
            # Ã–zel kelimeler (mÃ¼ÅŸteri destek baÄŸlamÄ±nda)
            urgency_words = ['acil', 'urgent', 'hemen', 'Ã§abuk', 'acilen']
            feature_dict['urgency_words'] = sum(1 for word in urgency_words if word in text.lower())
            
            polite_words = ['lÃ¼tfen', 'teÅŸekkÃ¼r', 'saygÄ±', 'merhaba', 'iyi gÃ¼nler']
            feature_dict['polite_words'] = sum(1 for word in polite_words if word in text.lower())
            
            negative_words = ['kÃ¶tÃ¼', 'berbat', 'sorun', 'hata', 'problem', 'ÅŸikayet']
            feature_dict['negative_words'] = sum(1 for word in negative_words if word in text.lower())
            
            features.append(feature_dict)
        
        df_features = pd.DataFrame(features)
        print(f"âœ… Ä°statistiksel: {len(df_features.columns)} Ã¶zellik Ã§Ä±karÄ±ldÄ±")
        return df_features
    
    def extract_lexical_features(self, texts):
        """Kelime bilgisi Ã¶zelliklerini Ã§Ä±karÄ±r"""
        print("ğŸ“š Kelime bilgisi Ã¶zellikleri Ã§Ä±karÄ±lÄ±yor...")
        
        features = []
        
        # TÃ¼m metinlerden kelime frekanslarÄ±
        all_words = []
        for text in texts:
            all_words.extend(text.lower().split())
        
        word_freq = Counter(all_words)
        vocabulary_size = len(word_freq)
        
        for text in texts:
            feature_dict = {}
            words = text.lower().split()
            
            if not words:
                features.append({key: 0 for key in ['lexical_diversity', 'rare_word_ratio', 
                               'common_word_ratio', 'unique_word_ratio']})
                continue
            
            # Kelime Ã§eÅŸitliliÄŸi (Type-Token Ratio)
            unique_words = set(words)
            feature_dict['lexical_diversity'] = len(unique_words) / len(words)
            
            # Nadir kelime oranÄ± (frekansÄ± < 5)
            rare_words = [word for word in words if word_freq[word] < 5]
            feature_dict['rare_word_ratio'] = len(rare_words) / len(words)
            
            # YaygÄ±n kelime oranÄ± (top %10)
            top_words = set([word for word, freq in word_freq.most_common(int(vocabulary_size * 0.1))])
            common_words = [word for word in words if word in top_words]
            feature_dict['common_word_ratio'] = len(common_words) / len(words)
            
            # Tekil kelime oranÄ±
            feature_dict['unique_word_ratio'] = len(unique_words) / len(words)
            
            features.append(feature_dict)
        
        df_features = pd.DataFrame(features)
        print(f"âœ… Kelime bilgisi: {len(df_features.columns)} Ã¶zellik Ã§Ä±karÄ±ldÄ±")
        return df_features
    
    def combine_all_features(self, texts, tfidf_max_features=3000):
        """TÃ¼m Ã¶zellik tiplerini birleÅŸtirir"""
        print("ğŸ”— TÃ¼m Ã¶zellikler birleÅŸtiriliyor...")
        
        # TF-IDF Ã¶zellikleri
        tfidf_matrix, tfidf_names = self.extract_tfidf_features(texts, max_features=tfidf_max_features)
        tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_names)
        
        # Ä°statistiksel Ã¶zellikler
        stat_features = self.extract_statistical_features(texts)
        
        # Kelime bilgisi Ã¶zellikleri
        lexical_features = self.extract_lexical_features(texts)
        
        # TÃ¼m Ã¶zellikleri birleÅŸtir
        combined_features = pd.concat([tfidf_df, stat_features, lexical_features], axis=1)
        
        print(f"ğŸ¯ Toplam {combined_features.shape[1]} Ã¶zellik oluÅŸturuldu")
        print(f"   - TF-IDF: {tfidf_df.shape[1]}")
        print(f"   - Ä°statistiksel: {stat_features.shape[1]}")
        print(f"   - Kelime bilgisi: {lexical_features.shape[1]}")
        
        return combined_features
    
    def transform_new_text(self, texts, feature_type='tfidf'):
        """EÄŸitilmiÅŸ vectorizer ile yeni metinleri dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r"""
        if feature_type == 'tfidf' and self.tfidf_vectorizer:
            return self.tfidf_vectorizer.transform(texts)
        elif feature_type == 'count' and self.count_vectorizer:
            return self.count_vectorizer.transform(texts)
        elif feature_type == 'hashing' and self.hashing_vectorizer:
            return self.hashing_vectorizer.transform(texts)
        else:
            raise ValueError(f"Vectorizer '{feature_type}' henÃ¼z eÄŸitilmemiÅŸ")
    
    def get_top_features(self, feature_matrix, feature_names, labels, n_top=10):
        """Her kategori iÃ§in en Ã¶nemli Ã¶zellikleri bulur"""
        print(f"ğŸ† Her kategori iÃ§in top {n_top} Ã¶zellik bulunuyor...")
        
        top_features = {}
        unique_labels = np.unique(labels)
        
        for label in unique_labels:
            # Bu kategoriye ait dokÃ¼manlarÄ± bul
            label_mask = (labels == label)
            label_features = feature_matrix[label_mask]
            
            # Ortalama TF-IDF skorlarÄ±
            mean_scores = np.mean(label_features, axis=0)
            
            # En yÃ¼ksek skorlu Ã¶zellikleri bul
            if hasattr(mean_scores, 'A1'):  # sparse matrix ise
                mean_scores = mean_scores.A1
            
            top_indices = np.argsort(mean_scores)[::-1][:n_top]
            top_feature_names = [feature_names[i] for i in top_indices]
            top_scores = [mean_scores[i] for i in top_indices]
            
            top_features[label] = list(zip(top_feature_names, top_scores))
        
        return top_features
    
    def print_top_features(self, top_features):
        """En Ã¶nemli Ã¶zellikleri yazdÄ±rÄ±r"""
        print("\nğŸ† KATEGORÄ° BAÅINA EN Ã–NEMLÄ° Ã–ZELLÄ°KLER")
        print("=" * 60)
        
        for category, features in top_features.items():
            print(f"\nğŸ“‚ {category.upper()}:")
            for i, (feature, score) in enumerate(features, 1):
                print(f"   {i:2d}. {feature:<20} ({score:.4f})")

    def extract_all_features(self, texts, max_tfidf_features=5000, max_count_features=5000):
        """TÃ¼m Ã¶zellik tÃ¼rlerini Ã§Ä±karÄ±r ve birleÅŸtirir"""
        print("ğŸ”§ TÃ¼m Ã¶zellikler Ã§Ä±karÄ±lÄ±yor...")
        
        # TF-IDF Ã¶zellikleri
        try:
            # Test iÃ§in daha dÃ¼ÅŸÃ¼k min_df kullan
            self.tfidf_vectorizer = TfidfVectorizer(
                max_features=max_tfidf_features,
                ngram_range=(1, 2),
                min_df=1,  # Test iÃ§in daha dÃ¼ÅŸÃ¼k threshold
                max_df=0.95,
                sublinear_tf=True,
                strip_accents='unicode'
            )
            tfidf_matrix = self.tfidf_vectorizer.fit_transform(texts)
            tfidf_feature_names = self.tfidf_vectorizer.get_feature_names_out()
        except ValueError as e:
            print(f"âš ï¸ TF-IDF hatasÄ±: {e}, varsayÄ±lan matris kullanÄ±lÄ±yor")
            tfidf_matrix = np.zeros((len(texts), 10))
            tfidf_feature_names = [f"tfidf_{i}" for i in range(10)]
        
        # Ä°statistiksel Ã¶zellikler
        stat_features = self.extract_statistical_features(texts)
        
        # Kelime bilgisi Ã¶zellikleri
        lexical_features = self.extract_lexical_features(texts)
        
        # Ã–zellikleri birleÅŸtir
        import scipy.sparse as sp
        if sp.issparse(tfidf_matrix):
            tfidf_dense = tfidf_matrix.toarray()
        else:
            tfidf_dense = tfidf_matrix
            
        combined_features = np.hstack([
            tfidf_dense,
            stat_features.values,
            lexical_features.values
        ])
        
        # Ã–zellik isimlerini birleÅŸtir
        all_feature_names = (
            list(tfidf_feature_names) + 
            list(stat_features.columns) + 
            list(lexical_features.columns)
        )
        
        print(f"âœ… Toplam {combined_features.shape[1]} Ã¶zellik Ã§Ä±karÄ±ldÄ±")
        return combined_features, all_feature_names

def demo_feature_extraction():
    """Ã–zellik Ã§Ä±karma demo'su"""
    print("ğŸ§ª Ã–zellik Ã‡Ä±karma Demo'su")
    print("=" * 50)
    
    # Ã–rnek metinler
    sample_texts = [
        "Kredi kartÄ±mdan para Ã§ekildi ama rezervasyonum onaylanmadÄ±! Ã‡ok acil.",
        "Åifremi unuttum, nasÄ±l deÄŸiÅŸtirebilirim? LÃ¼tfen yardÄ±m edin.",
        "Site Ã§ok yavaÅŸ yÃ¼kleniyor. Bu durumda Ã§ok Ã¼zgÃ¼nÃ¼m.",
        "Merhaba, Ã§alÄ±ÅŸma saatleriniz nedir? TeÅŸekkÃ¼rler.",
        "Personel Ã§ok kaba davrandÄ±. Åikayet etmek istiyorum!"
    ]
    
    labels = ["payment_issue", "user_error", "technical_issue", "general_info", "complaint"]
    
    # Feature extractor oluÅŸtur
    extractor = FeatureExtractor()
    
    # TF-IDF Ã¶zellikleri
    tfidf_matrix, tfidf_names = extractor.extract_tfidf_features(sample_texts, max_features=100)
    
    # En Ã¶nemli Ã¶zellikleri bul
    top_features = extractor.get_top_features(tfidf_matrix, tfidf_names, labels, n_top=5)
    extractor.print_top_features(top_features)
    
    # Ä°statistiksel Ã¶zellikler
    stat_features = extractor.extract_statistical_features(sample_texts)
    print(f"\nğŸ“Š Ä°statistiksel Ã–zellikler:")
    print(stat_features.head())

if __name__ == "__main__":
    demo_feature_extraction()
